{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New CML workflow query\n",
    "\n",
    "In this notebook, I retrieve the URLs of the CML workflows as suggested by the iterativeAI devs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import configparser\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from github import Github\n",
    "from github.PaginatedList import PaginatedList\n",
    "\n",
    "DATA_FOLDER: Path = Path(\"../data\")\n",
    "SLEEP_TIME: float = 2.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_rate_limit(github: Github):\n",
    "        \"\"\"Check the rate limit of the Github API.\"\"\"\n",
    "\n",
    "        core_rate_limit = github.get_rate_limit().core\n",
    "        if core_rate_limit.remaining <= 5:\n",
    "            print(\"Rate limit reached...\")\n",
    "            reset_timestamp = calendar.timegm(core_rate_limit.reset.timetuple())\n",
    "            # add 5 seconds to be sure the rate limit has been reset)\n",
    "            sleep_time = reset_timestamp - calendar.timegm(time.gmtime()) + 5\n",
    "            print(f\"Sleeping for {sleep_time} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "            print(\"Resuming after sleep...\")\n",
    "\n",
    "\n",
    "def load_gh_token(env_file_path: str = \"../env.ini\") -> str:\n",
    "    \"\"\"Load the GitHub token.\"\"\"\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "    env_file = Path(env_file_path)\n",
    "    if not env_file.exists():\n",
    "        raise ValueError(f\"The env file '{env_file_path}' does not exist.\")\n",
    "    else:\n",
    "        config.read(env_file)\n",
    "\n",
    "    return json.loads(config.get(\"GITHUB\", \"TOKEN_LIST\"))[0]\n",
    "\n",
    "def search_workflows(github: Github, extension: str) -> PaginatedList:\n",
    "    \"\"\"Perform a search code query on GitHub.\n",
    "    \n",
    "    It searches all workflow files in the repository's .github/workflows folder\n",
    "    with the given extension that use the `iterative/setup-cml` action.\n",
    "    \"\"\"\n",
    "    \n",
    "    QUERY = f'iterative/setup-cml path:.github/workflows extension:{extension}'\n",
    "    return github.search_code(QUERY)\n",
    "\n",
    "\n",
    "def extract_workflow_data(workflow_html_url: str) -> tuple[str, str]:\n",
    "    \"\"\"Extracts the repository URL and the workflow file name from the workflow URL.\n",
    "\n",
    "    For instance, given the URL:\n",
    "        `https://github.com/Etheredge-Works/gloves/blob/4d9bb10bdd294bcdd5213e6b8181ad83538cb22a/.github/workflows/training.yaml`\n",
    "\n",
    "    the function returns:\n",
    "        - Etheredge-Works/gloves\n",
    "        - training.yaml\n",
    "\n",
    "    Args:\n",
    "        workflow_html_url (str): the URL of the workflow file\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, str]: the repository URL and the workflow file name\n",
    "    \"\"\"\n",
    "\n",
    "    PATTERN = r\"https:\\/\\/github\\.com\\/(.*)\\/blob.*\\/.github/workflows/(.*)\"\n",
    "\n",
    "    match = re.search(PATTERN, workflow_html_url)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1), match.group(2)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Could not extract repository URL and workflow file name from URL: {workflow_html_url}\"\n",
    "        )\n",
    "\n",
    "def get_workflow_data(github: Github, query_result: PaginatedList, sleep_time: float = 1.0) -> dict:\n",
    "    \"\"\"Returns a dict containing information about the retrieved workflow files.\n",
    "    Args:\n",
    "        github (Github): the GitHub object used to connect to the API\n",
    "        query_result (PaginatedList): the query result to parse\n",
    "        sleep_time (float, optional): Time to wait after each request.\n",
    "            Defaults to 1, avoiding secondary rate limit errors.\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary with the repository URL as key and the list of relevant \n",
    "        workflow files as value.\n",
    "    \"\"\"\n",
    "\n",
    "    # workflows: dict = {}\n",
    "\n",
    "    # for file in query_result:\n",
    "        \n",
    "    #     time.sleep(sleep_time)\n",
    "    #     # check_rate_limit(github)\n",
    "        \n",
    "    #     # Add repository URL as key in dictionary and set value to empty list\n",
    "    #     repository_url = file.repository.html_url\n",
    "    #     if repository_url not in workflows:\n",
    "    #         workflows[repository_url] = []\n",
    "\n",
    "    #     # Add workflow file to list of workflows for repository\n",
    "    #     workflows[repository_url].append(file.name)\n",
    "\n",
    "    # return workflows\n",
    "\n",
    "    workflows: dict = {}\n",
    "\n",
    "    for file in query_result:\n",
    "        \n",
    "        time.sleep(sleep_time)\n",
    "        \n",
    "\n",
    "        workflow_url = file.html_url\n",
    "        repository_url, workflow_filename = extract_workflow_data(workflow_url)\n",
    "\n",
    "        # Add repository URL as key in dictionary and set value to empty list\n",
    "        if repository_url not in workflows:\n",
    "            workflows[repository_url] = []\n",
    "\n",
    "        # Add workflow file to list of workflows for repository\n",
    "        workflows[repository_url].append(workflow_filename)\n",
    "\n",
    "    return workflows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform queries on GitHub.\n",
    "\n",
    "Since a single query returns more than 1000 results, i.e., the maximum allowed by the GitHub API, we need to perform multiple queries to get all the results.\n",
    "By restricting the search on the file extension, we can be sure to retrieve all relevant files with two queries: one for `.yml` files and one for `.yaml` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_TOKEN = load_gh_token()\n",
    "github = Github(GITHUB_TOKEN)\n",
    "print(f\"Connected to GitHub as `{github.get_user().login}`\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1: .yml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSEARCHING `.yml` FILES...\")\n",
    "result = search_workflows(github, 'yml')\n",
    "print(f'Total number of retrieved files: {result.totalCount}')\n",
    "\n",
    "print(\"\\nRetrieving workflow data...\")\n",
    "yml_workflows = get_workflow_data(github, result, sleep_time=SLEEP_TIME)\n",
    "print(\"Workflow data retrieved.\")\n",
    "\n",
    "print(\"Writing workflow data to file...\")\n",
    "filename = \"yml_CML_workflows.json\"\n",
    "with open(DATA_FOLDER / filename, \"w\") as f:\n",
    "    json.dump(yml_workflows, f, indent=4)\n",
    "print(f\"Data written to file: `{filename}`.\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2: .yaml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSEARCHING `.yaml` FILES...\")\n",
    "result = search_workflows(github, 'yaml')\n",
    "print(f'Total number of retrieved files: {result.totalCount}')\n",
    "\n",
    "print(\"\\nRetrieving workflow data...\")\n",
    "yaml_workflows = get_workflow_data(github, result, sleep_time=SLEEP_TIME)\n",
    "print(\"Workflow data retrieved.\")\n",
    "\n",
    "print(\"Writing workflow data to file...\")\n",
    "filename = \"yaml_CML_workflows.json\"\n",
    "with open(DATA_FOLDER / filename, \"w\") as f:\n",
    "    json.dump(yml_workflows, f, indent=4)\n",
    "print(f\"Data written to file: `{filename}`.\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "merge = deepcopy(yml_workflows)\n",
    "for workflow in yaml_workflows:\n",
    "    if workflow in merge:\n",
    "        merge[workflow].extend(yaml_workflows[workflow])\n",
    "    else:\n",
    "        merge[workflow] = yaml_workflows[workflow]\n",
    "\n",
    "print(\"Writing workflow data to file...\")\n",
    "filename = \"merged_CML_workflows.json\"\n",
    "with open(DATA_FOLDER / filename, \"w\") as f:\n",
    "    json.dump(yml_workflows, f, indent=4)\n",
    "print(f\"Data written to file: `{filename}`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
